# Lokale Transkriptions-Backends

[üá∫üá∏ English Version](LOCAL_BACKENDS.md)

PulseScribe unterst√ºtzt Offline-Transkription mit lokalen Whisper-Modellen. Nach dem initialen Modell-Download sind keine API-Keys oder Internetverbindung erforderlich.

## Backend-Vergleich

| Backend | Geschwindigkeit | Plattform | GPU | Ideal f√ºr |
|---------|-----------------|-----------|-----|-----------|
| **lightning** | ‚ö°‚ö°‚ö°‚ö° | Apple Silicon | Metal | Maximale Geschwindigkeit (M1+) |
| **mlx** | ‚ö°‚ö°‚ö° | Apple Silicon | Metal | Stabilit√§t + Geschwindigkeit |
| **faster** | ‚ö°‚ö° | Alle | Nur CPU | CPU-only Systeme |
| **whisper** | ‚ö° | Alle | MPS/CUDA | Kompatibilit√§t |

## Schnellstart

### Apple Silicon (Empfohlen)

```bash
# MLX Backend installieren
pip install mlx-whisper

# Konfigurieren
export PULSESCRIBE_MODE=local
export PULSESCRIBE_LOCAL_BACKEND=mlx
export PULSESCRIBE_LOCAL_MODEL=turbo
export PULSESCRIBE_LANGUAGE=de

# Starten
python pulsescribe_daemon.py
```

### CPU-only Systeme

```bash
# faster-whisper installieren
pip install faster-whisper

# Konfigurieren
export PULSESCRIBE_MODE=local
export PULSESCRIBE_LOCAL_BACKEND=faster
export PULSESCRIBE_LOCAL_MODEL=turbo

# Starten
python pulsescribe_daemon.py
```

---

## Backend-Details

### Lightning (`lightning-whisper-mlx`)

**~4x schneller** als Standard-MLX durch Batched Decoding.

```bash
PULSESCRIBE_LOCAL_BACKEND=lightning
```

| Variable | Werte | Default | Beschreibung |
|----------|-------|---------|--------------|
| `PULSESCRIBE_LIGHTNING_BATCH_SIZE` | 6-24 | 12 | H√∂her = schneller, mehr RAM |
| `PULSESCRIBE_LIGHTNING_QUANT` | `4bit`, `8bit`, (leer) | (keiner) | Quantisierung f√ºr Speichereinsparung |

**Unterst√ºtzte Modelle:** `tiny`, `base`, `small`, `medium`, `large`, `large-v2`, `large-v3`

> **Hinweis:** `turbo` wird automatisch auf `large-v3` gemappt (turbo nicht in Lightning verf√ºgbar).

**Automatischer Fallback:** Bei Lightning-Fehlern f√§llt PulseScribe automatisch auf MLX zur√ºck.

---

### MLX (`mlx-whisper`)

Native Metal-Beschleunigung f√ºr Apple Silicon. Gute Balance aus Geschwindigkeit und Stabilit√§t.

```bash
pip install mlx-whisper
PULSESCRIBE_LOCAL_BACKEND=mlx
```

**Modellnamen-Zuordnung:**

| Kurzname | Vollst√§ndige Repo-ID |
|----------|---------------------|
| `turbo` | `mlx-community/whisper-large-v3-turbo` ‚≠ê |
| `large` | `mlx-community/whisper-large-v3-mlx` |
| `medium` | `mlx-community/whisper-medium` |
| `small` | `mlx-community/whisper-small-mlx` |
| `base` | `mlx-community/whisper-base-mlx` |
| `tiny` | `mlx-community/whisper-tiny` |

**Nur Englisch (destilliert, 30-40% schneller):**

| Kurzname | Vollst√§ndige Repo-ID |
|----------|---------------------|
| `large-en` | `mlx-community/distil-whisper-large-v3` |
| `medium-en` | `mlx-community/distil-whisper-medium.en` |
| `small-en` | `mlx-community/distil-whisper-small.en` |

> **Warnung:** `-en` Modelle unterst√ºtzen nur Englisch. F√ºr Deutsch/andere Sprachen `turbo` oder `large` verwenden.

**Einschr√§nkungen:**
- `PULSESCRIBE_LOCAL_BEAM_SIZE` wird ignoriert (Beam Search nicht implementiert)

---

### Faster-Whisper (`faster-whisper`)

CTranslate2-basiertes Backend. Sehr schnell auf CPU, geringerer Speicherbedarf.

```bash
pip install faster-whisper
PULSESCRIBE_LOCAL_BACKEND=faster
```

| Variable | Werte | Default | Beschreibung |
|----------|-------|---------|--------------|
| `PULSESCRIBE_LOCAL_COMPUTE_TYPE` | `int8`, `float16`, `float32` | `int8` (CPU) | Rechengenauigkeit |
| `PULSESCRIBE_LOCAL_CPU_THREADS` | 0-N | 0 (auto) | CPU-Threads (0 = alle Kerne) |
| `PULSESCRIBE_LOCAL_NUM_WORKERS` | 1-N | 1 | Parallele Worker |
| `PULSESCRIBE_LOCAL_WITHOUT_TIMESTAMPS` | `true`, `false` | `true` | Timestamps deaktivieren |
| `PULSESCRIBE_LOCAL_VAD_FILTER` | `true`, `false` | `false` | Voice Activity Detection |

**Hinweise:**
- Auf macOS nur CPU (keine Metal/MPS-Unterst√ºtzung)
- Modellname `turbo` wird zu `large-v3-turbo` gemappt
- Standard-`compute_type` ist `float16` auf CUDA

---

### OpenAI Whisper (`openai-whisper`)

Originale PyTorch-Implementierung. Beste Kompatibilit√§t, nutzt MPS auf Apple Silicon.

```bash
pip install openai-whisper
PULSESCRIBE_LOCAL_BACKEND=whisper
```

| Variable | Werte | Default | Beschreibung |
|----------|-------|---------|--------------|
| `PULSESCRIBE_DEVICE` | `auto`, `mps`, `cpu`, `cuda` | `auto` | Rechenger√§t |
| `PULSESCRIBE_FP16` | `true`, `false` | Auto | FP16-Genauigkeit erzwingen |

**Automatische Ger√§tewahl:**
- Apple Silicon ‚Üí MPS
- NVIDIA GPU ‚Üí CUDA
- Sonst ‚Üí CPU

---

## Performance-Tuning

### Schnelles Decoding

F√ºr Geschwindigkeit aktivieren (leichter Robustheitsverlust):

```bash
PULSESCRIBE_LOCAL_FAST=true
# Entspricht:
PULSESCRIBE_LOCAL_BEAM_SIZE=1
PULSESCRIBE_LOCAL_BEST_OF=1
PULSESCRIBE_LOCAL_TEMPERATURE=0.0
```

### Feintuning-Parameter

| Variable | Bereich | Default | Beschreibung |
|----------|---------|---------|--------------|
| `PULSESCRIBE_LOCAL_BEAM_SIZE` | 1-10 | 1 | Beam-Search-Breite |
| `PULSESCRIBE_LOCAL_BEST_OF` | 1-10 | 1 | Kandidaten pro Beam |
| `PULSESCRIBE_LOCAL_TEMPERATURE` | 0.0-1.0 | 0.0 | Sampling-Temperatur |

> **Hinweis:** H√∂here Werte = bessere Qualit√§t, langsamere Geschwindigkeit.

### Warmup

Erste-Nutzung-Latenz durch Modell-Vorladung reduzieren:

```bash
PULSESCRIBE_LOCAL_WARMUP=true   # Immer Warmup
PULSESCRIBE_LOCAL_WARMUP=false  # Nie Warmup
PULSESCRIBE_LOCAL_WARMUP=auto   # Default: Warmup f√ºr openai-whisper auf MPS
```

---

## Modellgr√∂√üen

| Modell | Parameter | VRAM | Geschwindigkeit | Qualit√§t |
|--------|-----------|------|-----------------|----------|
| `tiny` | 39M | ~1 GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ |
| `base` | 74M | ~1 GB | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ |
| `small` | 244M | ~2 GB | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ |
| `medium` | 769M | ~5 GB | ‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ |
| `large` | 1550M | ~10 GB | üê¢ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |
| `turbo` | 809M | ~6 GB | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ ‚≠ê |

‚≠ê **Empfohlen:** `turbo` f√ºr beste Geschwindigkeit/Qualit√§t-Balance.

---

## Modell-Cache-Speicherorte

| Backend | Cache-Pfad |
|---------|------------|
| `whisper` | `~/.cache/whisper/` |
| `faster-whisper` | `~/.cache/huggingface/` |
| `mlx-whisper` | `~/.cache/huggingface/` |
| `lightning` | `~/.pulsescribe/lightning_models/` |

**Festplattennutzung:** 75 MB (tiny) bis 3 GB (large) pro Modell.

---

## Systemvoraussetzungen

### Abh√§ngigkeiten

```bash
# macOS: Erforderlich f√ºr alle lokalen Backends
brew install ffmpeg portaudio

# Ubuntu/Debian
sudo apt install ffmpeg
```

> **Hinweis:** `ffmpeg` wird nur f√ºr Datei-Transkription ben√∂tigt, nicht f√ºr Live-Mikrofon-Aufnahme.

### Backend-spezifisch

| Backend | Installationsbefehl |
|---------|---------------------|
| `whisper` | `pip install openai-whisper` |
| `faster` | `pip install faster-whisper` |
| `mlx` | `pip install mlx-whisper` |
| `lightning` | `pip install lightning-whisper-mlx` |

---

## Fehlerbehebung

| Problem | L√∂sung |
|---------|--------|
| `ModuleNotFoundError: No module named 'mlx'` | Nur Apple Silicon. Auf Intel `faster` verwenden. |
| Modell-Download 404 | Kurznamen (`large`) oder vollst√§ndige Repo-ID verwenden |
| `beam_size not implemented (mlx)` | `PULSESCRIBE_LOCAL_BEAM_SIZE` entfernen |
| Langsame erste Transkription | `PULSESCRIBE_LOCAL_WARMUP=true` aktivieren |
| Speichermangel | Kleineres Modell oder `PULSESCRIBE_LIGHTNING_QUANT=4bit` |
| `Read-only file system` (DMG) | Modelle werden in `~/.pulsescribe/lightning_models/` gespeichert |

---

## Beispielkonfigurationen

### Maximale Geschwindigkeit (Apple Silicon)

```bash
PULSESCRIBE_MODE=local
PULSESCRIBE_LOCAL_BACKEND=lightning
PULSESCRIBE_LOCAL_MODEL=turbo
PULSESCRIBE_LOCAL_FAST=true
PULSESCRIBE_LIGHTNING_BATCH_SIZE=16
```

### Wenig Speicher (Apple Silicon)

```bash
PULSESCRIBE_MODE=local
PULSESCRIBE_LOCAL_BACKEND=mlx
PULSESCRIBE_LOCAL_MODEL=small
PULSESCRIBE_LIGHTNING_QUANT=4bit
```

### CPU-Server

```bash
PULSESCRIBE_MODE=local
PULSESCRIBE_LOCAL_BACKEND=faster
PULSESCRIBE_LOCAL_MODEL=medium
PULSESCRIBE_LOCAL_COMPUTE_TYPE=int8
PULSESCRIBE_LOCAL_CPU_THREADS=8
```

---

_Zuletzt aktualisiert: Dezember 2025_
