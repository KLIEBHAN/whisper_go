# PulseScribe

[![Tests](https://github.com/KLIEBHAN/pulsescribe/actions/workflows/test.yml/badge.svg)](https://github.com/KLIEBHAN/pulsescribe/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/KLIEBHAN/pulsescribe/graph/badge.svg)](https://codecov.io/gh/KLIEBHAN/pulsescribe)

[ðŸ‡©ðŸ‡ª Deutsche Version](README.de.md)

Voice input for macOS â€“ inspired by [Wispr Flow](https://wisprflow.ai). Transcribes audio using OpenAI Whisper via API, Deepgram, Groq, or locally.

**Features:** Real-time Streaming (Deepgram) Â· Multiple Providers (OpenAI, Deepgram, Groq, Local incl. MLX/Metal on Apple Silicon) Â· LLM Post-processing Â· Context Awareness Â· Custom Vocabulary Â· Live Preview Overlay Â· Menu Bar Feedback

> **Performance:** Ultra-fast startup with ~170ms to "Ready-Sound" thanks to parallel microphone and WebSocket initialization. Audio is transcribed during recording â€“ results appear immediately after stopping.

### Provider Overview

| Provider     | Latency   | Method    | Special Feature                                        |
| ------------ | --------- | --------- | ------------------------------------------------------ |
| **Deepgram** | ~300ms âš¡ | WebSocket | Real-time streaming, recommended                       |
| **Groq**     | ~1s       | REST      | Whisper on LPU, very fast                              |
| **OpenAI**   | ~2-3s     | REST      | GPT-4o, highest quality                                |
| **Local**    | varies    | Whisper   | Offline, no API costs (MLX/Lightning on Apple Silicon) |

## Quick Start

Ready to use in under 2 minutes:

```bash
# 1. Clone repository
git clone https://github.com/KLIEBHAN/pulsescribe.git && cd pulsescribe

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set API Key (Deepgram: $200 free credit)
export DEEPGRAM_API_KEY="your_key"

# 4. First recording
python transcribe.py --record --copy --mode deepgram
```

### Recommended `.env` Configuration

PulseScribe loads settings from `~/.pulsescribe/.env` (recommended; used by the Settings UI and the daemon).  
For development, a local `.env` in the project directory is also supported.

```bash
# Recommended (works for the daemon / app bundle)
cp .env.example ~/.pulsescribe/.env
```

Example `~/.pulsescribe/.env`:

```bash
# API Keys
DEEPGRAM_API_KEY=...
GROQ_API_KEY=...
OPENAI_API_KEY=...
OPENROUTER_API_KEY=...

# Transcription
PULSESCRIBE_MODE=deepgram
PULSESCRIBE_LANGUAGE=en

# LLM Post-processing
PULSESCRIBE_REFINE=true
PULSESCRIBE_REFINE_PROVIDER=groq
PULSESCRIBE_REFINE_MODEL=openai/gpt-oss-120b
```

**Why these settings?**

| Setting                            | Reason                                             |
| ---------------------------------- | -------------------------------------------------- |
| `MODE=deepgram`                    | Fastest option (~300ms) via WebSocket streaming    |
| `REFINE_PROVIDER=groq`             | Free/cheap LLM inference on LPU hardware           |
| `REFINE_MODEL=openai/gpt-oss-120b` | Open-source GPT alternative with excellent quality |
| `LANGUAGE=en`                      | Explicit language improves transcription accuracy  |

> **Tip:** For system-wide hotkeys see [Hotkey Integration](#hotkey-integration).

## CLI Usage

Two main functions: Transcribe audio files or record directly from the microphone.

### Transcribe Audio File

```bash
python transcribe.py audio.mp3                        # Default (API Mode)
python transcribe.py audio.mp3 --mode openai          # OpenAI GPT-4o Transcribe
python transcribe.py audio.mp3 --mode deepgram        # Deepgram Nova-3
python transcribe.py audio.mp3 --mode groq            # Groq (fastest option)
python transcribe.py audio.mp3 --mode local           # Offline with local Whisper
```

### Microphone Recording

```bash
python transcribe.py --record                         # Record and print output
python transcribe.py --record --copy                  # Copy directly to clipboard
python transcribe.py --record --refine                # With LLM post-processing
```

**Workflow:** Enter â†’ Speak â†’ Enter â†’ Transcript appears

### All Options

| Option                                 | Description                                                                   |
| -------------------------------------- | ----------------------------------------------------------------------------- |
| `--mode openai\|local\|deepgram\|groq` | Transcription provider (default: `openai`)                                    |
| `--model NAME`                         | Model (CLI > `PULSESCRIBE_MODEL` env > Provider default)                      |
| `--record`, `-r`                       | Microphone recording instead of file                                          |
| `--copy`, `-c`                         | Copy result to clipboard                                                      |
| `--language CODE`                      | Language code e.g., `de`, `en`                                                |
| `--format FORMAT`                      | Output: `text`, `json`, `srt`, `vtt` (only API mode)                          |
| `--refine`                             | Enable LLM post-processing                                                    |
| `--no-refine`                          | Disable LLM post-processing (overrides env)                                   |
| `--refine-model`                       | Model for post-processing (default: `openai/gpt-oss-120b`)                    |
| `--refine-provider`                    | LLM provider: `groq`, `openai` or `openrouter`                                |
| `--context`                            | Context for post-processing: `email`, `chat`, `code`, `default` (auto-detect) |

## Configuration

All settings can be set via environment variables or a `.env` file. CLI arguments always take precedence.

### API Keys

Depending on the selected mode, an API key is required:

```bash
# OpenAI (for --mode openai and --refine with openai)
export OPENAI_API_KEY="sk-..."

# Deepgram (for --mode deepgram) â€“ $200 free credit
export DEEPGRAM_API_KEY="..."

# Groq (for --mode groq and --refine with groq) â€“ free credits
export GROQ_API_KEY="gsk_..."

# OpenRouter (Alternative for --refine) â€“ Hundreds of models
export OPENROUTER_API_KEY="sk-or-..."
```

### Default Settings

```bash
# Transcription Mode (openai, local, deepgram, groq)
export PULSESCRIBE_MODE="deepgram"

# Transcription Model (overrides Provider default)
export PULSESCRIBE_MODEL="nova-3"

# Device for local Whisper (auto, mps, cpu, cuda)
# Default: auto â†’ uses MPS on Apple Silicon, otherwise CPU/CUDA
export PULSESCRIBE_DEVICE="auto"

# Force FP16 for local Whisper (true/false)
# Default: CPU/MPS â†’ false (stable), CUDA â†’ true
export PULSESCRIBE_FP16="false"

# Backend for local Whisper (whisper, faster, mlx, auto)
# whisper = openai-whisper (PyTorch, uses MPS/GPU)
# faster  = faster-whisper (CTranslate2, very fast on CPU)
# mlx     = mlx-whisper (MLX/Metal, Apple Silicon, optional)
# auto    = faster if installed, else whisper
export PULSESCRIBE_LOCAL_BACKEND="whisper"

# Local model override (only for local mode)
# Default: provider default (turbo)
# export PULSESCRIBE_LOCAL_MODEL="turbo"

# Compute type for faster-whisper (optional)
# Default: int8 on CPU, float16 on CUDA
# export PULSESCRIBE_LOCAL_COMPUTE_TYPE="int8"

# Faster-whisper threading (optional)
# 0 threads = auto (all cores)
# export PULSESCRIBE_LOCAL_CPU_THREADS=0
# export PULSESCRIBE_LOCAL_NUM_WORKERS=1

# Faster-whisper options (optional)
# default on faster: without_timestamps=true, vad_filter=false
# export PULSESCRIBE_LOCAL_WITHOUT_TIMESTAMPS="true"
# export PULSESCRIBE_LOCAL_VAD_FILTER="false"

# Optional: faster local decoding (more speed, slightly less robustness)
# Default: true on faster-whisper, false on openai-whisper
# export PULSESCRIBE_LOCAL_FAST="true"  # sets beam_size=1, best_of=1, temperature=0.0
# Fine-tuning:
# export PULSESCRIBE_LOCAL_BEAM_SIZE=1
# export PULSESCRIBE_LOCAL_BEST_OF=1
# export PULSESCRIBE_LOCAL_TEMPERATURE=0.0

# Optional: Local Warmup (reduces "cold start" on first local call)
# Default: auto (warmup only for openai-whisper on MPS). Values: true/false (unset = auto)
# export PULSESCRIBE_LOCAL_WARMUP="true"

# WebSocket Streaming for Deepgram (default: true)
export PULSESCRIBE_STREAMING="true"

# LLM Post-processing
export PULSESCRIBE_REFINE="true"
export PULSESCRIBE_REFINE_MODEL="openai/gpt-oss-120b"
export PULSESCRIBE_REFINE_PROVIDER="openai"  # or openrouter, groq
```

### System Dependencies

Certain modes require additional tools:

```bash
# Local Mode (file transcription)
# Required for `PULSESCRIBE_LOCAL_BACKEND=whisper` and `mlx` when transcribing audio files.
brew install ffmpeg          # macOS
sudo apt install ffmpeg      # Ubuntu/Debian

# Microphone Recording (macOS)
brew install portaudio
```

## Advanced Features

Beyond basic transcription, pulsescribe offers intelligent post-processing and customization.

### LLM Post-processing

Removes filler words (um, uh, like), corrects grammar, and formats into clean paragraphs:

```bash
python transcribe.py --record --refine
```

Supported Providers: OpenAI (default), [OpenRouter](https://openrouter.ai), [Groq](https://groq.com)

### Context Awareness

Post-processing automatically detects the active app and adjusts the writing style:

| Context   | Apps                      | Style                      |
| --------- | ------------------------- | -------------------------- |
| `email`   | Mail, Outlook, Spark      | Formal, complete sentences |
| `chat`    | Slack, Discord, Messages  | Casual, short and concise  |
| `code`    | VS Code, Cursor, Terminal | Technical, preserve terms  |
| `default` | All others                | Standard correction        |

```bash
# Automatic detection (Default)
python transcribe.py --record --refine

# Manual override
python transcribe.py --record --refine --context email

# Custom App Mappings
export PULSESCRIBE_APP_CONTEXTS='{"MyApp": "chat"}'
```

### Real-Time Audio Feedback

The overlay reacts in real-time to voice with a dynamic sound wave visualization:

- **Listening (ðŸŒ¸ Pink):** System waiting for voice input.
- **Recording (ðŸ”´ Red):** Voice detected, recording in progress. Bars visualize volume.
- **Transcribing (ðŸŸ  Orange):** Recording finished, processing text.

Thanks to integrated Voice Activity Detection (VAD), the status switches immediately when speaking begins.

### Voice Commands

Control formatting via spoken commands (automatically active with `--refine`):

| German           | English            | Result     |
| ---------------- | ------------------ | ---------- |
| "neuer Absatz"   | "new paragraph"    | Paragraph  |
| "neue Zeile"     | "new line"         | Line break |
| "Punkt"          | "period"           | `.`        |
| "Komma"          | "comma"            | `,`        |
| "Fragezeichen"   | "question mark"    | `?`        |
| "Ausrufezeichen" | "exclamation mark" | `!`        |
| "Doppelpunkt"    | "colon"            | `:`        |
| "Semikolon"      | "semicolon"        | `;`        |

```bash
# Example
python transcribe.py --record --refine
# Speak: "Hello comma how are you question mark"
# Result: "Hello, how are you?"
```

> **Note:** Voice commands are interpreted by the LLM â€“ they only work with `--refine`.

### Custom Vocabulary

Custom terms for better recognition in `~/.pulsescribe/vocabulary.json`:

```json
{
  "keywords": ["Anthropic", "Claude", "Kubernetes", "OAuth"]
}
```

Supported by Deepgram and local Whisper. The OpenAI API does not support custom vocabulary â€“ LLM post-processing helps there.

## Hotkey Integration

For system-wide voice input via hotkey â€“ the main use case of pulsescribe.

### Unified Daemon (Recommended)

The `pulsescribe_daemon.py` combines all components in one process:

- Hotkey Listener (QuickMacHotKey)
- Microphone Recording + Deepgram Streaming
- Menu Bar Status (ðŸŽ¤ ðŸ”´ â³ âœ… âŒ) - via `ui/menubar.py`
- Overlay with Animations - via `ui/overlay.py`
- Auto-Paste

```bash
# Manual Start
python pulsescribe_daemon.py

# With CLI Options
python pulsescribe_daemon.py --hotkey cmd+shift+r --debug

# As Login Item (Double click or add to Login Items)
open start_daemon.command
```

> **Toggle hotkeys don't need Accessibility permission.** QuickMacHotKey uses the native Carbon API (`RegisterEventHotKey`).  
> **Hold mode uses Quartz event taps and requires Input Monitoring** on macOS.

### Settings UI (Menu Bar)

Use the menu bar icon â†’ **Settings...** to configure provider keys, mode, local backend/model, and advanced local performance knobs (device, warmup, fast decoding, faster-whisper compute/threading, etc.).  
Settings are stored in `~/.pulsescribe/.env` and applied live (hotkey changes apply immediately).

### Configuration

In `.env` or as environment variable:

```bash
# Hotkeys (default: Fn/Globe as hold)
#
# Optional: toggle + hold in parallel.
# If set, these override PULSESCRIBE_HOTKEY / PULSESCRIBE_HOTKEY_MODE.
#
# Recommended default: use Fn/Globe as Pushâ€‘toâ€‘Talk (hold).
# PULSESCRIBE_HOLD_HOTKEY=fn
# Optional: add a separate toggle hotkey (e.g. F19).
# PULSESCRIBE_TOGGLE_HOTKEY=f19
#
# Legacy (single hotkey):
PULSESCRIBE_HOTKEY=fn
PULSESCRIBE_HOTKEY_MODE=hold

# Dock Icon (default: true) â€“ set to false for menubar-only mode
PULSESCRIBE_DOCK_ICON=true
```

**Supported Hotkeys:**

| Format          | Example                                 |
| --------------- | --------------------------------------- |
| Function Keys   | `f19`, `f1`, `f12`                      |
| Single Key      | `fn`, `capslock`, `space`, `tab`, `esc` |
| Key Combination | `cmd+shift+r`                           |

**Recommended hotkey setup (macOS):**

- **Fn/Globe as Holdâ€‘toâ€‘Record:** set `PULSESCRIBE_HOLD_HOTKEY=fn`.  
  This gives a fast, oneâ€‘finger Pushâ€‘toâ€‘Talk workflow. Requires Accessibility/Input Monitoring.
- **CapsLock alternative:** CapsLock can be used directly as a toggle hotkey, but macOS often toggles capitalization.  
  For a conflictâ€‘free â€œsingle key toggleâ€, map CapsLock â†’ `F19` via **Karabinerâ€‘Elements** and set `PULSESCRIBE_TOGGLE_HOTKEY=f19`.

### Usage

**Hold Mode (Default / Pushâ€‘toâ€‘Talk):**

- Hold Fn/Globe â†’ Recording runs while held
- Release Fn/Globe â†’ Transcript is pasted

**Toggle Mode (Optional, e.g. with F19):**

- Press F19 â†’ Recording starts
- Press F19 again â†’ Transcript is pasted

### Visual Feedback

The overlay shows the current status through colors and animations:

| Status           | Color     | Animation | Meaning                            |
| ---------------- | --------- | --------- | ---------------------------------- |
| **Listening**    | ðŸŒ¸ Pink   | Breathing | Hotkey pressed, waiting for speech |
| **Recording**    | ðŸ”´ Red    | Waves     | Speech detected, recording active  |
| **Transcribing** | ðŸŸ  Orange | Loading   | Finalizing transcription           |
| **Refining**     | ðŸ’œ Violet | Pulsing   | LLM post-processing active         |
| **Done**         | âœ… Green  | Bounce    | Done, text pasted                  |
| **Error**        | âŒ Red    | Blink     | Error occurred                     |

Both are integrated and start automatically with the daemon.

## Provider Comparison

| Mode       | Provider | Method    | Latency   | Special Feature                                            |
| ---------- | -------- | --------- | --------- | ---------------------------------------------------------- |
| `deepgram` | Deepgram | WebSocket | ~300ms âš¡ | Real-time streaming (recommended)                          |
| `groq`     | Groq     | REST      | ~1s       | Whisper on LPU, very fast                                  |
| `openai`   | OpenAI   | REST      | ~2-3s     | GPT-4o Transcribe, highest quality                         |
| `local`    | Whisper  | Local     | varies    | Offline, no API costs (Whisper / Faster / MLX / Lightning) |

> **Recommendation:** `--mode deepgram` for daily use. The streaming architecture ensures minimal waiting time between recording stop and text insertion.

### Local Backend Options

| Backend     | Description                                            | Best For            |
| ----------- | ------------------------------------------------------ | ------------------- |
| `whisper`   | OpenAI Whisper (PyTorch), works on all platforms       | Compatibility       |
| `faster`    | faster-whisper (CTranslate2), 4x faster on CPU         | CPU-only systems    |
| `mlx`       | mlx-whisper (Metal), optimized for Apple Silicon       | macOS stability     |
| `lightning` | lightning-whisper-mlx, ~4x faster via Batched Decoding | Maximum speed (M1+) |

Set via: `PULSESCRIBE_LOCAL_BACKEND=lightning`

**Lightning-specific options:**

- `PULSESCRIBE_LIGHTNING_BATCH_SIZE`: Batch size (default: 12, higher = faster, more RAM)
- `PULSESCRIBE_LIGHTNING_QUANT`: Quantization (`None`, `4bit`, `8bit`)

## Model Reference

### API Models (OpenAI)

| Model                    | Description      |
| ------------------------ | ---------------- |
| `gpt-4o-transcribe`      | Best quality â­  |
| `gpt-4o-mini-transcribe` | Faster, cheaper  |
| `whisper-1`              | Original Whisper |

### Deepgram Models

| Model    | Description                   |
| -------- | ----------------------------- |
| `nova-3` | Newest model, best quality â­ |
| `nova-2` | Proven model, cheaper         |

`smart_format` is activated â€“ automatic formatting of dates, currency, and paragraphs.

#### Real-time Streaming (Default)

Deepgram uses **WebSocket streaming** by default for minimal latency:

- Audio is transcribed **during recording**, not just afterwards
- Result appears **immediately** after stopping (instead of 2-3s wait)
- Ideal for hotkey integration

```bash
# Unified daemon uses streaming by default.
# REST fallback (daemon only):
PULSESCRIBE_STREAMING=false
```

### Groq Models

| Model                        | Description                          |
| ---------------------------- | ------------------------------------ |
| `whisper-large-v3`           | Whisper Large v3, ~300x real-time â­ |
| `distil-whisper-large-v3-en` | English only, even faster            |

Groq uses LPU chips (Language Processing Units) for particularly fast inference.

### Local Models

Local mode now supports three backends:

- **`whisper` (default):** openaiâ€‘whisper on PyTorch. Uses Appleâ€‘GPU via MPS automatically on Mâ€‘series Macs (`PULSESCRIBE_DEVICE=auto`). Best compatibility/quality.
- **`faster`:** fasterâ€‘whisper (CTranslate2). Very fast on CPU and lower memory. On macOS it runs on CPU (no MPS/Metal). Default `compute_type` is `int8` on CPU and `float16` on CUDA. Enable via `PULSESCRIBE_LOCAL_BACKEND=faster`.
- **`mlx`:** mlxâ€‘whisper (MLX/Metal). Apple Silicon GPUâ€‘accelerated local backend. Install with `pip install mlx-whisper` and enable via `PULSESCRIBE_LOCAL_BACKEND=mlx`.

Notes:

- Model name `turbo` maps to fasterâ€‘whisper `large-v3-turbo`.
- For maximum speed (with slight robustness tradeâ€‘off), set `PULSESCRIBE_LOCAL_FAST=true` or lower `PULSESCRIBE_LOCAL_BEAM_SIZE`/`PULSESCRIBE_LOCAL_BEST_OF`.
- For long recordings on `faster`, you can tune throughput via `PULSESCRIBE_LOCAL_CPU_THREADS` and `PULSESCRIBE_LOCAL_NUM_WORKERS`.
- For `mlx`, `PULSESCRIBE_LOCAL_BEAM_SIZE` is ignored (beam search is not implemented in mlxâ€‘whisper).

#### Quick setup (offline dictation)

Apple Silicon (recommended local backend):

```bash
pip install mlx-whisper
export PULSESCRIBE_MODE=local
export PULSESCRIBE_LOCAL_BACKEND=mlx
export PULSESCRIBE_LOCAL_MODEL=large   # or: turbo
export PULSESCRIBE_LANGUAGE=de         # optional
python pulsescribe_daemon.py --debug
```

#### Apple Silicon: MLX model names

With `PULSESCRIBE_LOCAL_BACKEND=mlx`, `PULSESCRIBE_LOCAL_MODEL` supports both short names and full Hugging Face repo IDs:

**Multilingual models (German, English, etc.):**

- `turbo` â†’ `mlx-community/whisper-large-v3-turbo` â­ (recommended for speed + quality)
- `large` â†’ `mlx-community/whisper-large-v3-mlx` (highest quality, slower)
- `medium` â†’ `mlx-community/whisper-medium`
- `small` â†’ `mlx-community/whisper-small-mlx`
- `base` â†’ `mlx-community/whisper-base-mlx`
- `tiny` â†’ `mlx-community/whisper-tiny`

**English-only (distilled, 30-40% faster, âš ï¸ ONLY English!):**

- `large-en` â†’ `mlx-community/distil-whisper-large-v3`
- `medium-en` â†’ `mlx-community/distil-whisper-medium.en`
- `small-en` â†’ `mlx-community/distil-whisper-small.en`

> **Note:** The `-en` models are distilled and only support English. For German or other languages, use `turbo` (best speed/quality) or `large` (best quality).

If you previously tried `whisper-large-v3` and hit a 404, use `large`/`large-v3` or the full repo ID `mlx-community/whisper-large-v3-mlx`.

#### Warmup / cold start

When running the daemon in `local` mode, the local model is preloaded in the background to reduce first-use latency.  
Optionally enable an additional warmup inference via `PULSESCRIBE_LOCAL_WARMUP=true` (most useful for `whisper` on MPS). If you start recording while warmup runs, nothing is â€œwastedâ€ â€” your first local transcription may just still include some cold-start overhead.

| Model  | Parameters | VRAM   | Speed          |
| ------ | ---------- | ------ | -------------- |
| tiny   | 39M        | ~1 GB  | Very fast      |
| base   | 74M        | ~1 GB  | Fast           |
| small  | 244M       | ~2 GB  | Medium         |
| medium | 769M       | ~5 GB  | Slow           |
| large  | 1550M      | ~10 GB | Very slow      |
| turbo  | 809M       | ~6 GB  | Fast & good â­ |

â­ = Default model of the provider

## Troubleshooting

| Problem                             | Solution                                                                                                                                                                |
| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Module not installed                | `pip install -r requirements.txt`                                                                                                                                       |
| API Key missing                     | `export DEEPGRAM_API_KEY="..."` (or OPENAI/GROQ)                                                                                                                        |
| Microphone issues (macOS)           | `brew install portaudio && pip install --force-reinstall sounddevice`                                                                                                   |
| Microphone permission               | Grant access in System Settings â†’ Privacy & Security â†’ Microphone                                                                                                       |
| ffmpeg missing                      | `brew install ffmpeg` (macOS) or `sudo apt install ffmpeg` (Ubuntu) â€” needed for local file transcription (`whisper`/`mlx`)                                             |
| MLX model download 404              | Use `PULSESCRIBE_LOCAL_MODEL=large` or a full repo ID (e.g. `mlx-community/whisper-large-v3-mlx`)                                                                       |
| Beam search not implemented (mlx)   | Remove `PULSESCRIBE_LOCAL_BEAM_SIZE` (ignored on `mlx`) or switch backend                                                                                               |
| Transcription slow                  | Switch to `--mode groq`/`deepgram`, or use `PULSESCRIBE_LOCAL_BACKEND=mlx` (Apple Silicon) / `faster` (CPU) and `PULSESCRIBE_LOCAL_FAST=true`, or a smaller local model |
| Daemon crashes silently             | Check `~/.pulsescribe/startup.log` for emergency logs                                                                                                                   |
| Auto-Paste not working (App Bundle) | See [Auto-Paste Troubleshooting](#auto-paste-troubleshooting-app-bundle)                                                                                                |

### Auto-Paste Troubleshooting (App Bundle)

If Auto-Paste doesn't work in `PulseScribe.app` (text is copied but not pasted):

**Clipboard behavior:** By default, the transcribed text stays in the clipboard after pasting. You can optionally restore your previous clipboard content:

```bash
# In ~/.pulsescribe/.env:
PULSESCRIBE_CLIPBOARD_RESTORE=true
```

When enabled, PulseScribe re-copies the previous text after a successful paste. This means clipboard history tools (Paste, Alfred, etc.) will see **both** entries: your transcription and the previous content.

If paste fails, the transcript stays in the clipboard so you can paste manually.

**Symptom:** Log shows `AXIsProcessTrusted = False` even though the app is enabled in Accessibility.

**Cause:** Unsigned PyInstaller bundles change their hash with every rebuild. macOS doesn't recognize the "new" app as authorized.

**Solution:**

1. System Settings â†’ Privacy & Security â†’ Accessibility
2. **Remove** `PulseScribe` (minus button)
3. **Re-add** `PulseScribe` (plus button or drag & drop the app)

> **Tip:** After every `pyinstaller build_app.spec`, you need to repeat this step as long as the app is not code-signed.

### Log Files

Logs are stored in `~/.pulsescribe/logs/`:

```bash
# Main log file
~/.pulsescribe/logs/pulsescribe.log

# Emergency startup log (if daemon fails to start)
~/.pulsescribe/startup.log
```

**Diagnostics report:** Menu bar â†’ **Export Diagnosticsâ€¦** creates a zip in `~/.pulsescribe/diagnostics/` (API keys masked, log tail redacted).

## Development

```bash
# Install test dependencies
pip install -r requirements-dev.txt

# Run tests
pytest -v

# With coverage report
pytest --cov=. --cov-report=term-missing
```

Tests run automatically via GitHub Actions on Push and Pull Requests.

### Building the macOS App Bundle

To create a standalone `PulseScribe.app`:

```bash
# Install PyInstaller (if not already installed)
pip install pyinstaller

# Build the app
pyinstaller build_app.spec

# Output: dist/PulseScribe.app
```

**Optional: Code-Sign for stable Accessibility permissions**

```bash
codesign --force --deep --sign - dist/PulseScribe.app
```

> **Note:** Without signing, you must re-authorize the app in System Settings â†’ Privacy & Security â†’ Accessibility after every rebuild. See [Auto-Paste Troubleshooting](#auto-paste-troubleshooting-app-bundle).

### Building a DMG (recommended for distribution)

```bash
# Dev (ad-hoc signed)
./build_dmg.sh

# Release (Developer ID + notarization)
export CODESIGN_IDENTITY="Developer ID Application: Your Name (TEAMID)"
export NOTARY_PROFILE="whispergo-notary"
./build_dmg.sh 1.0.0 --notarize
```

See `docs/BUILDING_MACOS.md` for notarization setup.
