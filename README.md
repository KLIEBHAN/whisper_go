# whisper_go

Einfaches CLI-Tool zum Transkribieren von Audio mit OpenAI Whisper ‚Äì sowohl √ºber die API als auch lokal.

## Features

- **API-Modus**: Nutzt OpenAI's Cloud-API (schnell, keine GPU n√∂tig)
- **Lokaler Modus**: Nutzt das Open-Source Whisper-Modell offline
- **Mikrofon-Aufnahme**: Enter dr√ºcken ‚Üí sprechen ‚Üí Enter ‚Üí fertig
- **Zwischenablage**: Transkript wird automatisch kopiert (bei `--record`)

## Installation

```bash
# Repository klonen / in Verzeichnis wechseln
cd whisper_go

# Dependencies installieren
pip install -r requirements.txt
```

### Zus√§tzliche Abh√§ngigkeiten

**F√ºr API-Modus (OpenAI):**

```bash
export OPENAI_API_KEY="dein_api_key"
```

**F√ºr Deepgram-Modus:**

```bash
export DEEPGRAM_API_KEY="dein_api_key"
```

> **Tipp:** Deepgram bietet 200$ Startguthaben f√ºr neue Accounts. [console.deepgram.com](https://console.deepgram.com)

**F√ºr Groq-Modus:**

```bash
export GROQ_API_KEY="gsk_..."
```

> **Tipp:** Groq bietet extrem schnelle Whisper-Inferenz (~300x Echtzeit) mit kostenlosen API-Credits. [console.groq.com](https://console.groq.com)

**Standard-Modus und Modell festlegen (optional):**

```bash
# In .env oder Shell-Config
export WHISPER_GO_MODE="deepgram"  # api, local, deepgram, oder groq
export WHISPER_GO_MODEL="nova-3"   # optional: √ºberschreibt Provider-Default
```

**LLM-Nachbearbeitung (Flow-Style):**

```bash
# In .env aktivieren
export WHISPER_GO_REFINE="true"

# Optional: OpenRouter statt OpenAI nutzen
export WHISPER_GO_REFINE_PROVIDER="openrouter"
export OPENROUTER_API_KEY="sk-or-..."

# Optional: Groq f√ºr schnelle LLM-Inferenz
export WHISPER_GO_REFINE_PROVIDER="groq"
export GROQ_API_KEY="gsk_..."

# Optional: Backend-Provider f√ºr OpenRouter festlegen
export OPENROUTER_PROVIDER_ORDER="Together,DeepInfra"
```

> Entfernt F√ºllw√∂rter (√§hm, also), korrigiert Grammatik und formatiert in saubere Abs√§tze. Unterst√ºtzt OpenAI (default), [OpenRouter](https://openrouter.ai) und [Groq](https://groq.com) f√ºr extrem schnelle Inferenz.

**Kontext-Awareness (automatisch):**

Die LLM-Nachbearbeitung erkennt automatisch die aktive App und passt den Schreibstil an:

| Kontext   | Apps                      | Stil                            |
| --------- | ------------------------- | ------------------------------- |
| `email`   | Mail, Outlook, Spark      | Formell, vollst√§ndige S√§tze     |
| `chat`    | Slack, Discord, Messages  | Locker, kurz und knapp          |
| `code`    | VS Code, Cursor, Terminal | Technisch, Begriffe beibehalten |
| `default` | Alle anderen              | Standard-Korrektur              |

```bash
# Automatische Erkennung (Standard)
python transcribe.py --record --refine

# Manueller Override
python transcribe.py --record --refine --context email

# Eigene App-Mappings (JSON)
export WHISPER_GO_APP_CONTEXTS='{"MyApp": "chat"}'
```

**Custom Vocabulary (Namen & Fachbegriffe):**

Eigene Begriffe f√ºr bessere Erkennung in `~/.whisper_go/vocabulary.json`:

```json
{
  "keywords": ["Anthropic", "Claude", "Kubernetes", "OAuth"]
}
```

Unterst√ºtzt von Deepgram (Nova-3: `keyterm`, Nova-2: `keywords`) und lokalem Whisper (`initial_prompt`). Die OpenAI API unterst√ºtzt kein Custom Vocabulary ‚Äì dort hilft nur die LLM-Nachbearbeitung.

**F√ºr lokalen Modus:**

```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
```

**F√ºr Mikrofon-Aufnahme (macOS):**

```bash
brew install portaudio
```

## Raycast Integration (Hotkey)

F√ºr systemweite Spracheingabe per Hotkey ‚Äì wie [Wispr Flow](https://wisprflow.ai):

```bash
cd whisper-go-raycast
npm install && npm run dev
```

**Setup in Raycast:**

1. "Toggle Recording" suchen
2. ‚åò+K ‚Üí "Assign Hotkey" ‚Üí **Double-Tap Right Option (‚å•‚å•)** empfohlen
3. Python-/Script-Pfad werden automatisch erkannt

**Nutzung (Toggle-Modus):**

- ‚å•‚å• (doppelt tippen) ‚Üí Aufnahme startet
- ‚å•‚å• (doppelt tippen) ‚Üí Transkript wird eingef√ºgt

### Push-to-Talk mit Karabiner-Elements

F√ºr echtes Push-to-Talk (Taste halten = Aufnahme, loslassen = einf√ºgen):

1. [Karabiner-Elements](https://karabiner-elements.pqrs.org/) installieren
2. Rule importieren:
   ```bash
   cp scripts/karabiner-ptt.json ~/.config/karabiner/assets/complex_modifications/
   ```
3. In Karabiner: Preferences ‚Üí Complex Modifications ‚Üí Add rule ‚Üí "Whisper Go Push-to-Talk"

**Nutzung (Push-to-Talk):**

- **Hyper+A** (‚åò‚åÉ‚å•‚áß+A) halten ‚Üí Aufnahme l√§uft
- **Hyper+A** loslassen ‚Üí Transkript wird eingef√ºgt

> **Tipp:** Caps Lock als Hyper-Key (‚åò‚åÉ‚å•‚áß) mappen, dann ist es nur Caps+A.
> Die Karabiner-Rule sendet F19. In Raycast muss F19 als Hotkey f√ºr "Toggle Recording" gesetzt sein.

### Men√ºbar-Feedback (optional)

Visuelles Feedback in der macOS-Men√ºleiste w√§hrend der Aufnahme:

```bash
# Installieren + Autostart einrichten (empfohlen)
./scripts/install_menubar.sh

# Oder manuell starten
pip install rumps
python menubar.py
```

| Icon | Status              |
| ---- | ------------------- |
| üé§   | Bereit (Idle)       |
| üî¥   | Aufnahme l√§uft      |
| ‚è≥   | Transkription l√§uft |
| ‚úÖ   | Erfolgreich         |
| ‚ùå   | Fehler              |

**Autostart-Verwaltung:**

```bash
./scripts/install_menubar.sh status    # Status pr√ºfen
./scripts/install_menubar.sh uninstall # Deinstallieren
```

> Die Men√ºbar-App zeigt nur den Status an. Start/Stop erfolgt weiterhin √ºber Raycast.

## CLI-Nutzung

### Audiodatei transkribieren

```bash
# Mit OpenAI API (default)
python transcribe.py audio.mp3

# Lokal
python transcribe.py audio.mp3 --mode local

# Lokal mit gr√∂√üerem Modell
python transcribe.py audio.mp3 --mode local --model large
```

### Mikrofon-Aufnahme

```bash
python transcribe.py --record
python transcribe.py --record --copy   # direkt in Zwischenablage
```

**Workflow:**

1. Enter dr√ºcken ‚Üí Aufnahme startet
2. Sprechen
3. Enter dr√ºcken ‚Üí Aufnahme stoppt
4. Transkript erscheint (mit `--copy` auch in Zwischenablage)

### Optionen

| Option                              | Beschreibung                                                                                     |
| ----------------------------------- | ------------------------------------------------------------------------------------------------ |
| `--mode api\|local\|deepgram\|groq` | API (default), lokales Whisper, Deepgram oder Groq                                               |
| `--record`, `-r`                    | Mikrofon-Aufnahme statt Datei                                                                    |
| `--copy`, `-c`                      | Ergebnis in Zwischenablage kopieren                                                              |
| `--model NAME`                      | Modellname (auch via `WHISPER_GO_MODEL` env). Defaults je Provider                               |
| `--language CODE`                   | Sprachcode z.B. `de`, `en`                                                                       |
| `--format FORMAT`                   | Output-Format: `text`, `json`, `srt`, `vtt` (nur API)                                            |
| `--refine`                          | LLM-Nachbearbeitung aktivieren (auch via `WHISPER_GO_REFINE` env)                                |
| `--no-refine`                       | LLM-Nachbearbeitung deaktivieren (√ºberschreibt env)                                              |
| `--refine-model MODEL`              | Modell f√ºr Nachbearbeitung (default: `gpt-5-nano`, auch via `WHISPER_GO_REFINE_MODEL` env)       |
| `--refine-provider`                 | LLM-Provider: `openai`, `openrouter` oder `groq` (auch via `WHISPER_GO_REFINE_PROVIDER` env)     |
| `--context`                         | Kontext f√ºr Nachbearbeitung: `email`, `chat`, `code`, `default` (auto-detect wenn nicht gesetzt) |

### Beispiele

```bash
# Einfachster Aufruf (API-Modus)
python transcribe.py audio.mp3

# Deutsche Sprache, SRT-Untertitel
python transcribe.py interview.mp3 --language de --format srt

# Schnelle lokale Transkription
python transcribe.py meeting.wav --mode local --model tiny

# Deepgram mit automatischer Formatierung
python transcribe.py audio.mp3 --mode deepgram --language de

# Aufnahme auf Deutsch, direkt in Zwischenablage
python transcribe.py --record --language de --copy
```

## Modell-Auswahl

### API-Modelle (OpenAI)

- `gpt-4o-transcribe` (Standard) ‚Äì Beste Qualit√§t
- `gpt-4o-mini-transcribe` ‚Äì Schneller, g√ºnstiger
- `whisper-1` ‚Äì Original Whisper

### Deepgram-Modelle

- `nova-3` (Standard) ‚Äì Neuestes Modell, beste Qualit√§t
- `nova-2` ‚Äì Bew√§hrtes Modell, etwas g√ºnstiger

**Features:** `smart_format` ist aktiviert ‚Äì automatische Formatierung von Datum, W√§hrung und Abs√§tzen.

### Groq-Modelle

- `whisper-large-v3` (Standard) ‚Äì OpenAI Whisper Large v3, extrem schnelle Inferenz (~300x Echtzeit)
- `distil-whisper-large-v3-en` ‚Äì Optimiert f√ºr Englisch, noch schneller

**Features:** Groq nutzt spezielle LPU-Chips (Language Processing Units) f√ºr besonders schnelle Inferenz.

### Lokale Modelle

| Modell | Parameter | VRAM   | Geschwindigkeit |
| ------ | --------- | ------ | --------------- |
| tiny   | 39M       | ~1 GB  | Sehr schnell    |
| base   | 74M       | ~1 GB  | Schnell         |
| small  | 244M      | ~2 GB  | Mittel          |
| medium | 769M      | ~5 GB  | Langsam         |
| large  | 1550M     | ~10 GB | Sehr langsam    |
| turbo  | 809M      | ~6 GB  | Schnell & gut   |

## Troubleshooting

**"Modul nicht installiert"**

```bash
pip install -r requirements.txt
```

**"OPENAI_API_KEY nicht gesetzt"**

```bash
export OPENAI_API_KEY="sk-..."
```

**Mikrofon funktioniert nicht (macOS)**

```bash
brew install portaudio
pip install --force-reinstall sounddevice
```

**ffmpeg fehlt**

```bash
# macOS
brew install ffmpeg

# Ubuntu
sudo apt install ffmpeg
```
