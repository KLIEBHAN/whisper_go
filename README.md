# whisper_go

[![Tests](https://github.com/KLIEBHAN/whisper_go/actions/workflows/test.yml/badge.svg)](https://github.com/KLIEBHAN/whisper_go/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/KLIEBHAN/whisper_go/graph/badge.svg)](https://codecov.io/gh/KLIEBHAN/whisper_go)

Spracheingabe f√ºr macOS ‚Äì inspiriert von [Wispr Flow](https://wisprflow.ai). Transkribiert Audio mit OpenAI Whisper √ºber API, Deepgram, Groq oder lokal.

**Features:** Echtzeit-Streaming (Deepgram) ¬∑ Mehrere Provider (OpenAI, Deepgram, Groq, lokal) ¬∑ LLM-Nachbearbeitung ¬∑ Kontext-Awareness ¬∑ Custom Vocabulary ¬∑ Raycast-Hotkeys ¬∑ Live-Preview Overlay ¬∑ Men√ºbar-Feedback

> **Performance:** Ultra-Fast-Startup mit ~170ms bis Ready-Sound dank parallelem Mikrofon- und WebSocket-Init. Audio wird w√§hrend der Aufnahme transkribiert ‚Äì Ergebnis erscheint sofort nach dem Stoppen.

## Schnellstart

In unter 2 Minuten einsatzbereit:

```bash
# 1. Repository klonen
git clone https://github.com/KLIEBHAN/whisper_go.git && cd whisper_go

# 2. Dependencies installieren
pip install -r requirements.txt

# 3. API-Key setzen (Deepgram: 200$ Startguthaben)
export DEEPGRAM_API_KEY="dein_key"

# 4. Erste Aufnahme
python transcribe.py --record --copy --mode deepgram
```

> **Tipp:** Kopiere `.env.example` nach `.env` f√ºr dauerhafte Konfiguration. F√ºr systemweite Hotkeys siehe [Raycast Integration](#raycast-integration).

## CLI-Nutzung

Zwei Hauptfunktionen: Audiodateien transkribieren oder direkt vom Mikrofon aufnehmen.

### Audiodatei transkribieren

```bash
python transcribe.py audio.mp3                        # Standard (API-Modus)
python transcribe.py audio.mp3 --mode deepgram        # Deepgram Nova-3
python transcribe.py audio.mp3 --mode groq            # Groq (schnellste Option)
python transcribe.py audio.mp3 --mode local           # Offline mit lokalem Whisper
```

### Mikrofon-Aufnahme

```bash
python transcribe.py --record                         # Aufnehmen und ausgeben
python transcribe.py --record --copy                  # Direkt in Zwischenablage
python transcribe.py --record --refine                # Mit LLM-Nachbearbeitung
```

**Workflow:** Enter ‚Üí Sprechen ‚Üí Enter ‚Üí Transkript erscheint

### Alle Optionen

| Option                              | Beschreibung                                                                  |
| ----------------------------------- | ----------------------------------------------------------------------------- |
| `--mode api\|local\|deepgram\|groq` | Transkriptions-Provider (default: `api`)                                      |
| `--model NAME`                      | Modell (CLI > `WHISPER_GO_MODEL` env > Provider-Default)                      |
| `--record`, `-r`                    | Mikrofon-Aufnahme statt Datei                                                 |
| `--copy`, `-c`                      | Ergebnis in Zwischenablage                                                    |
| `--language CODE`                   | Sprachcode z.B. `de`, `en`                                                    |
| `--format FORMAT`                   | Output: `text`, `json`, `srt`, `vtt` (nur API-Modus)                          |
| `--no-streaming`                    | WebSocket-Streaming deaktivieren (nur Deepgram)                               |
| `--refine`                          | LLM-Nachbearbeitung aktivieren                                                |
| `--no-refine`                       | LLM-Nachbearbeitung deaktivieren (√ºberschreibt env)                           |
| `--refine-model`                    | Modell f√ºr Nachbearbeitung (default: `gpt-5-nano`)                            |
| `--refine-provider`                 | LLM-Provider: `openai`, `openrouter`, `groq`                                  |
| `--context`                         | Kontext f√ºr Nachbearbeitung: `email`, `chat`, `code`, `default` (auto-detect) |

## Konfiguration

Alle Einstellungen k√∂nnen per Umgebungsvariable oder `.env`-Datei gesetzt werden. CLI-Argumente haben immer Vorrang.

### API-Keys

Je nach gew√§hltem Modus wird ein API-Key ben√∂tigt:

```bash
# OpenAI (f√ºr --mode api und --refine mit openai)
export OPENAI_API_KEY="sk-..."

# Deepgram (f√ºr --mode deepgram) ‚Äì 200$ Startguthaben
export DEEPGRAM_API_KEY="..."

# Groq (f√ºr --mode groq und --refine mit groq) ‚Äì kostenlose Credits
export GROQ_API_KEY="gsk_..."

# OpenRouter (Alternative f√ºr --refine) ‚Äì Hunderte Modelle
export OPENROUTER_API_KEY="sk-or-..."
```

### Standard-Einstellungen

```bash
# Transkriptions-Modus (api, local, deepgram, groq)
export WHISPER_GO_MODE="deepgram"

# Transkriptions-Modell (√ºberschreibt Provider-Default)
export WHISPER_GO_MODEL="nova-3"

# WebSocket-Streaming f√ºr Deepgram (default: true)
export WHISPER_GO_STREAMING="true"

# LLM-Nachbearbeitung
export WHISPER_GO_REFINE="true"
export WHISPER_GO_REFINE_MODEL="gpt-5-nano"
export WHISPER_GO_REFINE_PROVIDER="openai"  # oder openrouter, groq
```

### System-Abh√§ngigkeiten

F√ºr bestimmte Modi werden zus√§tzliche Tools ben√∂tigt:

```bash
# Lokaler Modus (ffmpeg f√ºr Audio-Konvertierung)
brew install ffmpeg          # macOS
sudo apt install ffmpeg      # Ubuntu/Debian

# Mikrofon-Aufnahme (macOS)
brew install portaudio
```

## Erweiterte Features

√úber die Basis-Transkription hinaus bietet whisper_go intelligente Nachbearbeitung und Anpassung.

### LLM-Nachbearbeitung

Entfernt F√ºllw√∂rter (√§hm, also, quasi), korrigiert Grammatik und formatiert in saubere Abs√§tze:

```bash
python transcribe.py --record --refine
```

Unterst√ºtzte Provider: OpenAI (default), [OpenRouter](https://openrouter.ai), [Groq](https://groq.com)

### Kontext-Awareness

Die Nachbearbeitung erkennt automatisch die aktive App und passt den Schreibstil an:

| Kontext   | Apps                      | Stil                            |
| --------- | ------------------------- | ------------------------------- |
| `email`   | Mail, Outlook, Spark      | Formell, vollst√§ndige S√§tze     |
| `chat`    | Slack, Discord, Messages  | Locker, kurz und knapp          |
| `code`    | VS Code, Cursor, Terminal | Technisch, Begriffe beibehalten |
| `default` | Alle anderen              | Standard-Korrektur              |

```bash
# Automatische Erkennung (Standard)
python transcribe.py --record --refine

# Manueller Override
python transcribe.py --record --refine --context email

# Eigene App-Mappings
export WHISPER_GO_APP_CONTEXTS='{"MyApp": "chat"}'
```

### Custom Vocabulary

Eigene Begriffe f√ºr bessere Erkennung in `~/.whisper_go/vocabulary.json`:

```json
{
  "keywords": ["Anthropic", "Claude", "Kubernetes", "OAuth"]
}
```

Unterst√ºtzt von Deepgram und lokalem Whisper. Die OpenAI API unterst√ºtzt kein Custom Vocabulary ‚Äì dort hilft die LLM-Nachbearbeitung.

## Raycast Integration

F√ºr systemweite Spracheingabe per Hotkey ‚Äì der Hauptanwendungsfall von whisper_go.

### Setup

```bash
cd whisper-go-raycast
npm install && npm run dev
```

In Raycast:

1. "Toggle Recording" suchen
2. ‚åò+K ‚Üí "Assign Hotkey" ‚Üí **Double-Tap Right Option (‚å•‚å•)** empfohlen

### Nutzung

- ‚å•‚å• ‚Üí Aufnahme startet (Audio wird bereits live transkribiert!)
- ‚å•‚å• ‚Üí Transkript wird sofort eingef√ºgt (kein Warten auf API)

### Push-to-Talk (optional)

F√ºr echtes Push-to-Talk (Taste halten = Aufnahme, loslassen = einf√ºgen) mit [Karabiner-Elements](https://karabiner-elements.pqrs.org/):

```bash
cp scripts/karabiner-ptt.json ~/.config/karabiner/assets/complex_modifications/
```

In Karabiner: Preferences ‚Üí Complex Modifications ‚Üí Add rule ‚Üí "Whisper Go Push-to-Talk"

**Nutzung:** Hyper+A halten ‚Üí Aufnahme ‚Üí Hyper+A loslassen ‚Üí Einf√ºgen

> **Tipp:** Caps Lock als Hyper-Key (‚åò‚åÉ‚å•‚áß) mappen, dann ist es nur Caps+A.

### Visuelles Feedback (optional)

Zwei Optionen f√ºr Status-Anzeige w√§hrend der Aufnahme:

#### Overlay (empfohlen)

Elegantes Overlay am unteren Bildschirmrand mit animierter Schallwellen-Visualisierung:

```bash
./scripts/install_overlay.sh
```

- Zeigt Live-Transkription w√§hrend dem Sprechen
- Animierte Schallwellen zeigen aktive Aufnahme
- Click-Through ‚Äì st√∂rt nicht beim Arbeiten
- Blur-Effekt wie bei Raycast

#### Men√ºbar

Kompaktes Icon in der macOS-Men√ºleiste:

```bash
./scripts/install_menubar.sh
```

| Icon | Status              |
| ---- | ------------------- |
| üé§   | Bereit              |
| üî¥   | Aufnahme l√§uft      |
| ‚è≥   | Transkription l√§uft |
| ‚úÖ   | Erfolgreich         |
| ‚ùå   | Fehler              |

> **Tipp:** Beide k√∂nnen gleichzeitig laufen. Das Overlay zeigt mehr Details, die Men√ºbar ist dezenter.

## Provider-Vergleich

| Modus      | Provider | Methode   | Latenz    | Besonderheit                        |
| ---------- | -------- | --------- | --------- | ----------------------------------- |
| `deepgram` | Deepgram | WebSocket | ~300ms ‚ö° | Echtzeit-Streaming (empfohlen)      |
| `groq`     | Groq     | REST      | ~1s       | Whisper auf LPU, sehr schnell       |
| `api`      | OpenAI   | REST      | ~2-3s     | GPT-4o Transcribe, h√∂chste Qualit√§t |
| `local`    | Whisper  | Lokal     | ~5-10s    | Offline, keine API-Kosten           |

> **Empfehlung:** `--mode deepgram` f√ºr den t√§glichen Gebrauch. Die Streaming-Architektur sorgt f√ºr minimale Wartezeit zwischen Aufnahme-Stopp und Text-Einf√ºgen.

## Modell-Referenz

### API-Modelle (OpenAI)

| Modell                   | Beschreibung         |
| ------------------------ | -------------------- |
| `gpt-4o-transcribe`      | Beste Qualit√§t ‚≠ê    |
| `gpt-4o-mini-transcribe` | Schneller, g√ºnstiger |
| `whisper-1`              | Original Whisper     |

### Deepgram-Modelle

| Modell   | Beschreibung                       |
| -------- | ---------------------------------- |
| `nova-3` | Neuestes Modell, beste Qualit√§t ‚≠ê |
| `nova-2` | Bew√§hrtes Modell, g√ºnstiger        |

`smart_format` ist aktiviert ‚Äì automatische Formatierung von Datum, W√§hrung und Abs√§tzen.

#### Echtzeit-Streaming (Standard)

Deepgram nutzt standardm√§√üig **WebSocket-Streaming** f√ºr minimale Latenz:

- Audio wird **w√§hrend der Aufnahme** transkribiert, nicht erst danach
- Ergebnis erscheint **sofort** nach dem Stoppen (statt 2-3s Wartezeit)
- Ideal f√ºr die Raycast-Integration

```bash
# Streaming (Standard)
python transcribe.py --record --mode deepgram

# REST-Fallback (falls Streaming Probleme macht)
python transcribe.py --record --mode deepgram --no-streaming
# oder via ENV:
WHISPER_GO_STREAMING=false
```

### Groq-Modelle

| Modell                       | Beschreibung                        |
| ---------------------------- | ----------------------------------- |
| `whisper-large-v3`           | Whisper Large v3, ~300x Echtzeit ‚≠ê |
| `distil-whisper-large-v3-en` | Nur Englisch, noch schneller        |

Groq nutzt LPU-Chips (Language Processing Units) f√ºr besonders schnelle Inferenz.

### Lokale Modelle

| Modell | Parameter | VRAM   | Geschwindigkeit  |
| ------ | --------- | ------ | ---------------- |
| tiny   | 39M       | ~1 GB  | Sehr schnell     |
| base   | 74M       | ~1 GB  | Schnell          |
| small  | 244M      | ~2 GB  | Mittel           |
| medium | 769M      | ~5 GB  | Langsam          |
| large  | 1550M     | ~10 GB | Sehr langsam     |
| turbo  | 809M      | ~6 GB  | Schnell & gut ‚≠ê |

‚≠ê = Standard-Modell des Providers

## Troubleshooting

| Problem                     | L√∂sung                                                                |
| --------------------------- | --------------------------------------------------------------------- |
| Modul nicht installiert     | `pip install -r requirements.txt`                                     |
| API-Key fehlt               | `export DEEPGRAM_API_KEY="..."` (oder OPENAI/GROQ)                    |
| Mikrofon geht nicht (macOS) | `brew install portaudio && pip install --force-reinstall sounddevice` |
| ffmpeg fehlt                | `brew install ffmpeg` (macOS) oder `sudo apt install ffmpeg` (Ubuntu) |
| Transkription langsam       | Wechsel zu `--mode groq` oder `--mode deepgram` statt `local`         |

## Development

```bash
# Test-Dependencies installieren
pip install -r requirements-dev.txt

# Tests ausf√ºhren
pytest -v

# Mit Coverage-Report
pytest --cov=. --cov-report=term-missing
```

Tests laufen automatisch via GitHub Actions bei Push und Pull Requests.
